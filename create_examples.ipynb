{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "from hashlib import sha256\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../lared')\n",
    "from helpers import file_functions\n",
    "from dataset.example import VideoExample, AudioExample, FullExample\n",
    "from constants.laughter_constants import data_path, cloud_data_path\n",
    "from data_loading.utils import get_video_caps, AudioFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "get_video_caps() takes 0 positional arguments but 1 was given",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1b491c111364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVideoExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_video_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/hdd/data/lared/concat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_video_caps() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "VideoExample.init_caps(get_video_caps('/mnt/hdd/data/lared/concat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughter_ann_path = os.path.join(cloud_data_path, 'laughter.txt')\n",
    "camera_ann_path = os.path.join(cloud_data_path, 'camera.txt')\n",
    "out_path = os.path.join(cloud_data_path, 'examples')\n",
    "images_path = os.path.join(out_path, 'images')\n",
    "\n",
    "\n",
    "laughter_ann = pd.read_csv(laughter_ann_path, sep='\\t',names=['name','participant','ini','fin','dur','type'],header=None)\n",
    "camera_ann = pd.read_csv(camera_ann_path, sep='\\t',names=['name','participant','ini','fin','dur','camera'],header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       name  participant       ini       fin     dur  camera\n",
       "0  camera30         32.0     0.000    10.000  10.000       0\n",
       "1   camera3          3.0   866.280   882.440  16.160       1\n",
       "2   camera3          3.0   883.150   884.490   1.340      12\n",
       "3   camera3          3.0   884.600   888.860   4.260     123\n",
       "4   camera3          3.0  4300.139  4300.956   0.817       4"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>participant</th>\n      <th>ini</th>\n      <th>fin</th>\n      <th>dur</th>\n      <th>camera</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>camera30</td>\n      <td>32.0</td>\n      <td>0.000</td>\n      <td>10.000</td>\n      <td>10.000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>camera3</td>\n      <td>3.0</td>\n      <td>866.280</td>\n      <td>882.440</td>\n      <td>16.160</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>camera3</td>\n      <td>3.0</td>\n      <td>883.150</td>\n      <td>884.490</td>\n      <td>1.340</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>camera3</td>\n      <td>3.0</td>\n      <td>884.600</td>\n      <td>888.860</td>\n      <td>4.260</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>camera3</td>\n      <td>3.0</td>\n      <td>4300.139</td>\n      <td>4300.956</td>\n      <td>0.817</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "camera_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_camera_by_participant = camera_ann.groupby('participant')\n",
    "df_laughter_by_participant = laughter_ann.groupby('participant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_camera = []\n",
    "for i in df_camera_by_participant.groups:\n",
    "    dfs_camera.append(df_camera_by_participant.get_group(i))\n",
    "dfs_laughter = []\n",
    "for i in df_laughter_by_participant.groups:\n",
    "    dfs_laughter.append(df_laughter_by_participant.get_group(i))\n",
    "participant_ids = df_camera_by_participant.groups.keys()\n",
    "participant_ids = [int(id) for id in participant_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera(camerastr):\n",
    "    if int(camerastr) == 0:\n",
    "        return []\n",
    "    \n",
    "    return [int(c) for c in str(camerastr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the camera where a person is seen\n",
    "def find_cameras(cam, t):\n",
    "    previous = cam[cam['fin'] < t]\n",
    "    if len(previous) == 0:\n",
    "        return []\n",
    "    \n",
    "    row = previous.iloc[-1]\n",
    "    cameras = row.tail()['camera']\n",
    "    return get_camera(cameras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laughter examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'FullExample' object has no attribute 'write_full_first_image'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d8cf216d4325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             })\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhashid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_full_first_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'FullExample' object has no attribute 'write_full_first_image'"
     ]
    }
   ],
   "source": [
    "# Create examples and first images\n",
    "examples = dict()\n",
    "for pid, df_laughter, df_camera in zip(participant_ids, dfs_laughter, dfs_camera):\n",
    "    for index,row in df_laughter.iterrows():\n",
    "        cameras = find_cameras(df_camera, row['ini'])\n",
    "        ini_time = (row['ini'])\n",
    "        fin_time = (row['fin'])\n",
    "\n",
    "        for cam in cameras:\n",
    "\n",
    "            hashstr = '_'.join([str(pid), str(cam), str(ini_time), str(fin_time)])\n",
    "            hashid = sha256(hashstr.encode()).hexdigest()\n",
    "            \n",
    "            examples[hashid] = FullExample({\n",
    "                'id': hashid,\n",
    "                'pid': pid,\n",
    "                'cam': cam,\n",
    "                'ini_time': ini_time,\n",
    "                'fin_time': fin_time\n",
    "            })\n",
    "\n",
    "            examples[hashid].write_full_first_image(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_segments(df_camera, pid, segment_len=10):\n",
    "    examples = list()\n",
    "    it = df_camera.iterrows()\n",
    "    prev = next(it)[1]\n",
    "    for index, row in it:\n",
    "        ini = prev.loc['fin'] + 10\n",
    "        fin = row.loc['ini'] - 10\n",
    "        cameras = get_camera(prev.loc['camera'])\n",
    "\n",
    "        for cam in cameras:\n",
    "            t = ini\n",
    "            while t < fin:\n",
    "                if t < 14*60:\n",
    "                    t += segment_len\n",
    "                    continue\n",
    "\n",
    "                hashstr = '_'.join([str(pid), str(cam), str(t), str(t+segment_len)])\n",
    "                hashid = sha256(hashstr.encode()).hexdigest()\n",
    "                examples.append(FullExample({\n",
    "                    'id': hashid,\n",
    "                    'pid': pid,\n",
    "                    'cam': cam,\n",
    "                    'ini_time': t,\n",
    "                    'fin_time': t + segment_len\n",
    "                }))\n",
    "                t += segment_len\n",
    "        prev = row\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_examples = [create_random_segments(df, pid) for pid, df in zip(participant_ids, dfs_camera)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_examples = [ex for e in random_examples for ex in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample examples and put them in order of pid (for easy labeling)\n",
    "random.seed(22)\n",
    "chosen_examples = random.sample(random_examples, 2000)\n",
    "chosen_examples = sorted(chosen_examples, key=lambda ex: ex.pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-04b356135b92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchosen_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_full_first_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/furnace/lared/dataset/example.py\u001b[0m in \u001b[0;36mwrite_full_first_image\u001b[0;34m(self, fout_path)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_video_ini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ex in tqdm(chosen_examples[:2000]):\n",
    "    ex.write_full_first_image(images_path)"
   ]
  },
  {
   "source": [
    "# Read CVAT annotations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_dict = {ex.id: ex for ex in random_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvat_bb_path = os.path.join(cloud_data_path, 'examples/pilot/cvat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the BBs from cvat annotations\n",
    "# Read camera and bounding box from it\n",
    "with open(cvat_bb_path) as json_file:\n",
    "    bbs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of img_id -> img\n",
    "imgs_dict = dict()\n",
    "for img in bbs['images']:\n",
    "    imgs_dict[img['id']] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over annotations and write media\n",
    "lared_examples = list()\n",
    "for annot in bbs['annotations']:\n",
    "    annot_img = imgs_dict[annot['image_id']]\n",
    "    example_id = annot_img['file_name'].split('_')[1]\n",
    "    example = examples_dict[example_id]\n",
    "    lared_examples.append(example)\n",
    "\n",
    "    bbox = annot['bbox']\n",
    "    example.rect = [int(e) for e in bbox]"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# AMI examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from ami_example import AmiExample\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_annotations = '/Users/josedvq/data/AMI/ami_public_manual_1.6.2/words'\n",
    "AmiExample.corpus_path = '/Users/josedvq/data/AMI/amicorpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetings_to_use = ['IS1002c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations\n",
    "meeting_annotations = dict()\n",
    "part_letters = ['', 'A', 'B', 'C', 'D']\n",
    "for meeting in meetings_to_use:\n",
    "    meeting_annotations[meeting] = dict()\n",
    "    for part_num in [1, 2, 3, 4]:\n",
    "        root = ET.parse(os.path.join(ami_annotations, f'{meeting}.{part_letters[part_num]}.words.xml')).getroot()\n",
    "        meeting_annotations[meeting][part_num] = {\n",
    "            'ini': float(root[0].attrib['starttime']),\n",
    "            'end': float(root[-1].attrib['endtime']),\n",
    "            'annot': [{\n",
    "                'type': t.tag,\n",
    "                'ini': float(t.attrib['starttime']), \n",
    "                'end': float(t.attrib['endtime']),\n",
    "                'content': t.text} \n",
    "                for t in root]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_segment(annot, length=10):\n",
    "    ini = int(random.random() * (annot['end'] - annot['ini'] - length))\n",
    "    return ini, ini + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40d306661dab4aed97cf6b43a58b29c6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# example generation\n",
    "N = 3\n",
    "for i in trange(0,N):\n",
    "    # pick a random meeting\n",
    "    meeting = random.choice(meetings_to_use)\n",
    "    random.sample([1,2,3,4], k=1)\n",
    "    participant = random.choice([1,2,3,4])\n",
    "\n",
    "    # pick a random meeting segment\n",
    "    ini, end = pick_random_segment(meeting_annotations[meeting][participant])\n",
    "    ex = AmiExample(meeting, participant, ini, end)\n",
    "    ex.write_video('test')\n",
    "    ex.write_audio('test')\n",
    "    ex.write_audiovisual('test', 'test', 'test')"
   ]
  },
  {
   "source": [
    "# Create covfee files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    \"type\": \"QuestionnaireTask\",\n",
    "    \"form\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"prompt\": \"Arousal: \",\n",
    "                \"input\": {\n",
    "                  \"type\": \"Slider\",\n",
    "                  \"defaultValue\": 50\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"media\": {\n",
    "        \"type\": \"video\",\n",
    "        \"autoplay\": False,\n",
    "        \"controls\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tasks = []\n",
    "audio_tasks = []\n",
    "av_tasks = []\n",
    "for example in lared_examples:\n",
    "    t = copy.deepcopy(task)\n",
    "    t['media']['url'] = 'pilot/video/' + example.get_file_id() + '.mp4'\n",
    "    t['instructions'] = 'Find the person indicated with a red rectangle. Play the video and pay attention to this person. Give the required ratings on the right.'\n",
    "    video_tasks.append(t)\n",
    "\n",
    "    t = copy.deepcopy(task)\n",
    "    t['media'] = {\n",
    "        'type': 'audio',\n",
    "        'url': 'pilot/audio/' + example.get_file_id() + '.wav'\n",
    "    }\n",
    "    t['instructions'] = 'Play the audio, paying attention to the main speaker. Then give the required ratings on the right.'\n",
    "    audio_tasks.append(t)\n",
    "\n",
    "    t = copy.deepcopy(task)\n",
    "    t['media']['url'] = 'pilot/av/' + example.get_file_id() + '.mp4'\n",
    "    t['instructions'] = 'Find the person indicated with a red rectangle. Play the video and pay attention to this person. Give the required ratings on the right.'\n",
    "    av_tasks.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = {\n",
    "    \"name\": \"Pilot\",\n",
    "    \"id\": \"pilot\",\n",
    "    \"email\": \"example@example.com\",\n",
    "    \"hits\": [\n",
    "        {\n",
    "            \"id\": \"pilot\",\n",
    "            \"name\": \"Pilot\",\n",
    "            \"type\": \"timeline\",\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"type\": \"VideoInstructionsTask\"\n",
    "                },\n",
    "                *video_tasks,\n",
    "                {\n",
    "                    \"type\": \"AudioInstructionsTask\"\n",
    "                },\n",
    "                *audio_tasks,\n",
    "                {\n",
    "                    \"type\": \"AVInstructionsTask\"\n",
    "                },\n",
    "                *av_tasks\n",
    "            ]\n",
    "                \n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(project, open('pilot.covfee.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}