{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning\n",
    "import pytorchvideo.data\n",
    "import pytorchvideo.models.resnet\n",
    "import pytorchvideo.models.slowfast\n",
    "import torch.utils.data\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from fvcore.common.config import CfgNode\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import kinetics_data_path, cloud_data_path\n",
    "import optimizer\n",
    "from dataset import my_video_dataset, my_video_dataset_from_dataframe\n",
    "from defaults import get_cfg\n",
    "from transforms import get_kinetics_train_transform, get_kinetics_val_transform\n",
    "from utils import get_metrics\n",
    "cfg = get_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kinetics_resnet():\n",
    "#   return pytorchvideo.models.slowfast.create_slowfast(\n",
    "#       input_channels=(3,3), # RGB input from Kinetics\n",
    "#       model_depth=50, # For the tutorial let's just use a 50 layer network\n",
    "#       model_num_class=2, # Kinetics has 400 classes so we need out final head to align\n",
    "#   )\n",
    "  return pytorchvideo.models.resnet.create_resnet(\n",
    "      input_channel=3, # RGB input from Kinetics\n",
    "      model_depth=50, # For the tutorial let's just use a 50 layer network\n",
    "      model_num_class=2, # Kinetics has 400 classes so we need out final head to align\n",
    "      norm=nn.BatchNorm3d,\n",
    "      activation=nn.ReLU,\n",
    "      \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet_feature_extractor():\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.blocks[-1].proj = nn.Linear(2048, 2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_slowfast_feature_extractor():\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.blocks[-1].proj = nn.Linear(2304, 2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
    "  def __init__(self, optim_cfg={}):\n",
    "      super().__init__()\n",
    "      self.model = make_slowfast_feature_extractor()\n",
    "      self.optim_cfg = optim_cfg\n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "\n",
    "      # learning rate scheduling\n",
    "      epoch_exact = self.current_epoch + float(batch_idx) / self.trainer.num_training_batches\n",
    "      self.log(\"epoch_exact\", epoch_exact)\n",
    "      lr = optimizer.get_epoch_lr(epoch_exact, self.optim_cfg)\n",
    "      \n",
    "      optimizer.set_lr(self.optimizers().optimizer, lr)\n",
    "      self.log(\"learning_rate\", lr)\n",
    "\n",
    "      # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
    "      # format provided by the dataset\n",
    "      y_hat = self.model(batch[\"video\"])\n",
    "\n",
    "      # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
    "      # by PyTorchLightning after being returned from this method.\n",
    "      loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
    "\n",
    "      # Log the train loss to Tensorboard\n",
    "      self.log(\"train_loss\", loss.item())\n",
    "\n",
    "      return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "      y_hat = self.model(batch[\"video\"])\n",
    "      loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
    "      self.log(\"val_loss\", loss)\n",
    "      return (y_hat[:,1], batch[\"label\"])\n",
    "\n",
    "  def validation_epoch_end(self, validation_step_outputs):\n",
    "        all_outputs = torch.cat([o[0] for o in validation_step_outputs]).cpu()\n",
    "        all_labels = torch.cat([o[1] for o in validation_step_outputs]).cpu()\n",
    "\n",
    "        try:\n",
    "            val_auc = roc_auc_score(all_labels, all_outputs)\n",
    "            self.log('val_auc', val_auc)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "      \"\"\"\n",
    "      Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
    "      usually useful for training video models.\n",
    "      \"\"\"\n",
    "      return optimizer.get_optimizer(self.model, self.optim_cfg)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_from_scratch():\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_other_cfg(CfgNode({\n",
    "        'SOLVER': {\n",
    "            'OPTIMIZING_METHOD': 'sgd',\n",
    "            'BASE_LR': 0.1,\n",
    "            'LR_POLICY': 'cosine',\n",
    "            'MOMENTUM': 0.9,\n",
    "            'WEIGHT_DECAY': 1e-4,\n",
    "            'WARMUP_EPOCHS': 0.0,\n",
    "            'WARMUP_START_LR': 0.01\n",
    "        }\n",
    "    }))\n",
    "    classification_module = VideoClassificationLightningModule(optim_cfg=cfg)\n",
    "    data_module = KineticsDataModule()\n",
    "    trainer = pytorch_lightning.Trainer(\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")],\n",
    "        accelerator='gpu',\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=50\n",
    "    )\n",
    "    trainer.fit(classification_module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fold(train_ds, test_ds):\n",
    "    # data loaders\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=8, shuffle=True, num_workers=10,\n",
    "        collate_fn=None)\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        test_ds, batch_size=8, shuffle=False, num_workers=10,\n",
    "        collate_fn=None)\n",
    "\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_other_cfg(CfgNode({\n",
    "        'SOLVER': {\n",
    "            'OPTIMIZING_METHOD': 'sgd',\n",
    "            'BASE_LR': 0.1,\n",
    "            'LR_POLICY': 'none',\n",
    "            'MOMENTUM': 0.9,\n",
    "            'WEIGHT_DECAY': 1e-4,\n",
    "        }\n",
    "    }))\n",
    "    \n",
    "    system = VideoClassificationLightningModule(optim_cfg=cfg)\n",
    "    trainer = pytorch_lightning.Trainer(\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")],\n",
    "        accelerator='gpu',\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=50)\n",
    "    trainer.fit(system, data_loader_train, data_loader_val)\n",
    "\n",
    "    trainer.test(system, data_loader_val)\n",
    "    return system.test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_validation(dataset, metrics_name='binary'):\n",
    "    seed = 22\n",
    "    cv_splits = KFold(n_splits=10, random_state=seed, shuffle=True).split(range(len(dataset)))\n",
    "\n",
    "    outputs = torch.empty((len(dataset),))\n",
    "    for f, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "        # create datasets    \n",
    "        train_ds = Subset(dataset, train_idx)\n",
    "        test_ds = Subset(dataset, test_idx)\n",
    "\n",
    "        fold_outputs = do_fold(train_ds, test_ds)\n",
    "        outputs[test_idx] = fold_outputs['proba'].cpu()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    labels = torch.Tensor(dataset.get_all_labels())\n",
    "    run_metrics = get_metrics(outputs, labels, metrics_name)\n",
    "\n",
    "    return outputs, run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = pd.read_csv('../dataset/computational_examples.csv')\n",
    "examples = examples[examples['condition'] == 'video']\n",
    "examples['filename'] = examples['hash']+'.mp4'\n",
    "video_path = os.path.join(cloud_data_path, 'laughter_data', 'ml_datasets', 'tight', 'video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>person</th>\n",
       "      <th>cam</th>\n",
       "      <th>hit_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>calibration</th>\n",
       "      <th>hash</th>\n",
       "      <th>ini_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>...</th>\n",
       "      <th>gt_laughter</th>\n",
       "      <th>is_laughter</th>\n",
       "      <th>confidence</th>\n",
       "      <th>intensity</th>\n",
       "      <th>attempt</th>\n",
       "      <th>pressed_key</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>rating_hash</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>9c45e4f0c5442e796eb93e73e94dc6c2dfca7b9c4c54ff...</td>\n",
       "      <td>video</td>\n",
       "      <td>False</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "      <td>7360.29</td>\n",
       "      <td>7361.54</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.336670</td>\n",
       "      <td>6.639973</td>\n",
       "      <td>7af591213b827db95c12c56e76e0b1fe518f2088d11aad...</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>bff9b86d833a595e6fe5a54f45093fa168cda45db1143e...</td>\n",
       "      <td>video</td>\n",
       "      <td>False</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "      <td>7360.29</td>\n",
       "      <td>7361.54</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.569236</td>\n",
       "      <td>6.639973</td>\n",
       "      <td>25df21dc0f25e11a7c4aba77e502269d42a7bb548044f2...</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>979</td>\n",
       "      <td>979</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>f4c9842cec7be99eeaaea36d0c7d077c4d5d94596dc731...</td>\n",
       "      <td>video</td>\n",
       "      <td>False</td>\n",
       "      <td>11bc9d8aca57ab2aef4c5305b080fa49c08665d9e94190...</td>\n",
       "      <td>2216.02</td>\n",
       "      <td>2216.54</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e56c0edb6a0870ea94b570507d246cb56c9ab3b1919a05...</td>\n",
       "      <td>11bc9d8aca57ab2aef4c5305b080fa49c08665d9e94190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2744</td>\n",
       "      <td>2744</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>eecc0cf5d634ce45a98cbbda30c922f2a2cfcb1877124c...</td>\n",
       "      <td>video</td>\n",
       "      <td>False</td>\n",
       "      <td>11bc9d8aca57ab2aef4c5305b080fa49c08665d9e94190...</td>\n",
       "      <td>2216.02</td>\n",
       "      <td>2216.54</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4c6a7bdcbec4e912d140f9a9ae4a196e0b55f0e5eff6b3...</td>\n",
       "      <td>11bc9d8aca57ab2aef4c5305b080fa49c08665d9e94190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9c45e4f0c5442e796eb93e73e94dc6c2dfca7b9c4c54ff...</td>\n",
       "      <td>video</td>\n",
       "      <td>False</td>\n",
       "      <td>c1d181e74dbdbce1e51d7d0bfd6e036913896dd1f22856...</td>\n",
       "      <td>3346.30</td>\n",
       "      <td>3347.70</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.736069</td>\n",
       "      <td>3.203203</td>\n",
       "      <td>07d4ee10402ea059d0a3791fd35fbaab20149aeb6ffb99...</td>\n",
       "      <td>c1d181e74dbdbce1e51d7d0bfd6e036913896dd1f22856...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.1  Unnamed: 0  person  cam  \\\n",
       "0              0           0      25    1   \n",
       "1           1947        1947      25    1   \n",
       "6            979         979      35    3   \n",
       "7           2744        2744      35    3   \n",
       "12             2           2       1    4   \n",
       "\n",
       "                                               hit_id condition  calibration  \\\n",
       "0   9c45e4f0c5442e796eb93e73e94dc6c2dfca7b9c4c54ff...     video        False   \n",
       "1   bff9b86d833a595e6fe5a54f45093fa168cda45db1143e...     video        False   \n",
       "6   f4c9842cec7be99eeaaea36d0c7d077c4d5d94596dc731...     video        False   \n",
       "7   eecc0cf5d634ce45a98cbbda30c922f2a2cfcb1877124c...     video        False   \n",
       "12  9c45e4f0c5442e796eb93e73e94dc6c2dfca7b9c4c54ff...     video        False   \n",
       "\n",
       "                                                 hash  ini_time  end_time  \\\n",
       "0   1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...   7360.29   7361.54   \n",
       "1   1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...   7360.29   7361.54   \n",
       "6   11bc9d8aca57ab2aef4c5305b080fa49c08665d9e94190...   2216.02   2216.54   \n",
       "7   11bc9d8aca57ab2aef4c5305b080fa49c08665d9e94190...   2216.02   2216.54   \n",
       "12  c1d181e74dbdbce1e51d7d0bfd6e036913896dd1f22856...   3346.30   3347.70   \n",
       "\n",
       "    ...  gt_laughter  is_laughter  confidence  intensity  attempt  \\\n",
       "0   ...         True         True           7          4        0   \n",
       "1   ...         True         True           1          6        0   \n",
       "6   ...         True        False           3          4        0   \n",
       "7   ...         True        False           5          4        0   \n",
       "12  ...         True         True           2          2        0   \n",
       "\n",
       "    pressed_key     onset    offset  \\\n",
       "0          True  3.336670  6.639973   \n",
       "1          True  2.569236  6.639973   \n",
       "6         False       NaN       NaN   \n",
       "7         False       NaN       NaN   \n",
       "12         True  2.736069  3.203203   \n",
       "\n",
       "                                          rating_hash  \\\n",
       "0   7af591213b827db95c12c56e76e0b1fe518f2088d11aad...   \n",
       "1   25df21dc0f25e11a7c4aba77e502269d42a7bb548044f2...   \n",
       "6   e56c0edb6a0870ea94b570507d246cb56c9ab3b1919a05...   \n",
       "7   4c6a7bdcbec4e912d140f9a9ae4a196e0b55f0e5eff6b3...   \n",
       "12  07d4ee10402ea059d0a3791fd35fbaab20149aeb6ffb99...   \n",
       "\n",
       "                                             filename  \n",
       "0   1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...  \n",
       "1   1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...  \n",
       "6   11bc9d8aca57ab2aef4c5305b080fa49c08665d9e94190...  \n",
       "7   11bc9d8aca57ab2aef4c5305b080fa49c08665d9e94190...  \n",
       "12  c1d181e74dbdbce1e51d7d0bfd6e036913896dd1f22856...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = my_video_dataset_from_dataframe(\n",
    "    examples_df=examples,\n",
    "    video_path=video_path,\n",
    "    video_path_prefix=video_path,\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", 2),\n",
    "    transform=get_kinetics_train_transform(32, 256, True),\n",
    "    decode_audio=False,\n",
    "    file_path_key='filename',\n",
    "    label_key='pressed_key'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jose/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | Net  | 33.6 M\n",
      "-------------------------------\n",
      "4.6 K     Trainable params\n",
      "33.6 M    Non-trainable params\n",
      "33.6 M    Total params\n",
      "134.596   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn 0, non bn 2, zero 0 no grad 330\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8905ee64195f44cf831c2fa0debe1ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3168972791714b14beccecdfa3439e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5434b614e5ea4f75a8dd82d2b1b89bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6414612a484900a6d9b929d6677b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d87069fb1e493ca8698945cd737543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7c830007094c7a90d2f67ad7ba849e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0924d1b96f24d0dbef015a59a49cafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b70dceab424e01b92724f8361af500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "No `test_step()` method defined to run `Trainer.test`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000032vscode-remote?line=0'>1</a>\u001b[0m do_cross_validation(dataset, metrics_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbinary\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb Cell 13'\u001b[0m in \u001b[0;36mdo_cross_validation\u001b[0;34m(dataset, metrics_name)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000016vscode-remote?line=7'>8</a>\u001b[0m train_ds \u001b[39m=\u001b[39m Subset(dataset, train_idx)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000016vscode-remote?line=8'>9</a>\u001b[0m test_ds \u001b[39m=\u001b[39m Subset(dataset, test_idx)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000016vscode-remote?line=10'>11</a>\u001b[0m fold_outputs \u001b[39m=\u001b[39m do_fold(train_ds, test_ds)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000016vscode-remote?line=11'>12</a>\u001b[0m outputs[test_idx] \u001b[39m=\u001b[39m fold_outputs[\u001b[39m'\u001b[39m\u001b[39mproba\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000016vscode-remote?line=12'>13</a>\u001b[0m clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb Cell 12'\u001b[0m in \u001b[0;36mdo_fold\u001b[0;34m(train_ds, test_ds)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000015vscode-remote?line=21'>22</a>\u001b[0m trainer \u001b[39m=\u001b[39m pytorch_lightning\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000015vscode-remote?line=22'>23</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m)],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000015vscode-remote?line=23'>24</a>\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000015vscode-remote?line=24'>25</a>\u001b[0m     log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000015vscode-remote?line=25'>26</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000015vscode-remote?line=26'>27</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(system, data_loader_train, data_loader_val)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000015vscode-remote?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(system, data_loader_val)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared-laughter/video/train.ipynb#ch0000015vscode-remote?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m system\u001b[39m.\u001b[39mtest_results\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:936\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=909'>910</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=910'>911</a>\u001b[0m \u001b[39mPerform one evaluation epoch over the test set.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=911'>912</a>\u001b[0m \u001b[39mIt's separated from fit to make sure you never run on your test set until you want to.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=932'>933</a>\u001b[0m \u001b[39m    The length of the list corresponds to the number of test dataloaders used.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=933'>934</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=934'>935</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[0;32m--> <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=935'>936</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=718'>719</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=719'>720</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:983\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=979'>980</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tested_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path  \u001b[39m# TODO: remove in v1.8\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=981'>982</a>\u001b[0m \u001b[39m# run test\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=982'>983</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=984'>985</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=985'>986</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1158\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1154'>1155</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_callbacks()\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1155'>1156</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_logging_functions()\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1157'>1158</a>\u001b[0m verify_loop_configurations(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1159'>1160</a>\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1160'>1161</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: preparing data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:46\u001b[0m, in \u001b[0;36mverify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=43'>44</a>\u001b[0m     __verify_eval_loop_configuration(trainer, model, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=44'>45</a>\u001b[0m \u001b[39melif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mTESTING:\n\u001b[0;32m---> <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=45'>46</a>\u001b[0m     __verify_eval_loop_configuration(trainer, model, \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=46'>47</a>\u001b[0m \u001b[39melif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mPREDICTING:\n\u001b[1;32m     <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=47'>48</a>\u001b[0m     __verify_eval_loop_configuration(trainer, model, \u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:210\u001b[0m, in \u001b[0;36m__verify_eval_loop_configuration\u001b[0;34m(trainer, model, stage)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=204'>205</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=205'>206</a>\u001b[0m     \u001b[39m# -----------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=206'>207</a>\u001b[0m     \u001b[39m# verify model has an eval_step\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=207'>208</a>\u001b[0m     \u001b[39m# -----------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=208'>209</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_step:\n\u001b[0;32m--> <a href='file:///home/jose/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py?line=209'>210</a>\u001b[0m         \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo `\u001b[39m\u001b[39m{\u001b[39;00mstep_name\u001b[39m}\u001b[39;00m\u001b[39m()` method defined to run `Trainer.\u001b[39m\u001b[39m{\u001b[39;00mtrainer_method\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: No `test_step()` method defined to run `Trainer.test`."
     ]
    }
   ],
   "source": [
    "do_cross_validation(dataset, metrics_name='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
