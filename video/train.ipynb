{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning\n",
    "import pytorchvideo.data\n",
    "import pytorchvideo.models.resnet\n",
    "import pytorchvideo.models.slowfast\n",
    "import torch.utils.data\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from fvcore.common.config import CfgNode\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lared_laughter.constants import dataset_path\n",
    "import optimizer\n",
    "from dataset import my_video_dataset_from_dataframe\n",
    "from defaults import get_cfg\n",
    "from transforms import get_kinetics_train_transform, get_kinetics_val_transform\n",
    "from utils import get_metrics\n",
    "cfg = get_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kinetics_resnet():\n",
    "#   return pytorchvideo.models.slowfast.create_slowfast(\n",
    "#       input_channels=(3,3), # RGB input from Kinetics\n",
    "#       model_depth=50, # For the tutorial let's just use a 50 layer network\n",
    "#       model_num_class=2, # Kinetics has 400 classes so we need out final head to align\n",
    "#   )\n",
    "  return pytorchvideo.models.resnet.create_resnet(\n",
    "      input_channel=3, # RGB input from Kinetics\n",
    "      model_depth=50, # For the tutorial let's just use a 50 layer network\n",
    "      model_num_class=2, # Kinetics has 400 classes so we need out final head to align\n",
    "      norm=nn.BatchNorm3d,\n",
    "      activation=nn.ReLU,\n",
    "      \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
    "  def __init__(self, optim_cfg={}):\n",
    "      super().__init__()\n",
    "      self.model = make_slowfast_feature_extractor()\n",
    "      self.optim_cfg = optim_cfg\n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "\n",
    "      # learning rate scheduling\n",
    "      epoch_exact = self.current_epoch + float(batch_idx) / self.trainer.num_training_batches\n",
    "      self.log(\"epoch_exact\", epoch_exact)\n",
    "      lr = optimizer.get_epoch_lr(epoch_exact, self.optim_cfg)\n",
    "      \n",
    "      optimizer.set_lr(self.optimizers().optimizer, lr)\n",
    "      self.log(\"learning_rate\", lr)\n",
    "\n",
    "      # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
    "      # format provided by the dataset\n",
    "      y_hat = self.model(batch[\"video\"])\n",
    "\n",
    "      # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
    "      # by PyTorchLightning after being returned from this method.\n",
    "      loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
    "\n",
    "      # Log the train loss to Tensorboard\n",
    "      self.log(\"train_loss\", loss.item())\n",
    "\n",
    "      return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "      y_hat = self.model(batch[\"video\"])\n",
    "      loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
    "      self.log(\"val_loss\", loss)\n",
    "      return (y_hat[:,1], batch[\"label\"])\n",
    "\n",
    "  def validation_epoch_end(self, validation_step_outputs):\n",
    "        all_outputs = torch.cat([o[0] for o in validation_step_outputs]).cpu()\n",
    "        all_labels = torch.cat([o[1] for o in validation_step_outputs]).cpu()\n",
    "\n",
    "        try:\n",
    "            val_auc = roc_auc_score(all_labels, all_outputs)\n",
    "            self.log('val_auc', val_auc)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "      y_hat = self.model(batch[\"video\"])\n",
    "      loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
    "      return (y_hat[:,1], batch[\"label\"])\n",
    "\n",
    "  def test_epoch_end(self, validation_step_outputs):\n",
    "        all_outputs = torch.cat([o[0] for o in validation_step_outputs]).cpu()\n",
    "        all_labels = torch.cat([o[1] for o in validation_step_outputs]).cpu()\n",
    "\n",
    "        self.test_results = {'proba': all_outputs, 'labels': all_labels}\n",
    "        try:\n",
    "            test_auc = roc_auc_score(all_labels, all_outputs)\n",
    "            self.test_results['auc'] = test_auc\n",
    "            self.log('test_auc', test_auc)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "      \"\"\"\n",
    "      Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
    "      usually useful for training video models.\n",
    "      \"\"\"\n",
    "      return optimizer.get_optimizer(self.model, self.optim_cfg)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_from_scratch():\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_other_cfg(CfgNode({\n",
    "        'SOLVER': {\n",
    "            'OPTIMIZING_METHOD': 'sgd',\n",
    "            'BASE_LR': 0.1,\n",
    "            'LR_POLICY': 'cosine',\n",
    "            'MOMENTUM': 0.9,\n",
    "            'WEIGHT_DECAY': 1e-4,\n",
    "            'WARMUP_EPOCHS': 0.0,\n",
    "            'WARMUP_START_LR': 0.01\n",
    "        }\n",
    "    }))\n",
    "    classification_module = VideoClassificationLightningModule(optim_cfg=cfg)\n",
    "    data_module = KineticsDataModule()\n",
    "    trainer = pytorch_lightning.Trainer(\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")],\n",
    "        accelerator='gpu',\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=50\n",
    "    )\n",
    "    trainer.fit(classification_module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fold(train_ds, test_ds):\n",
    "    # data loaders\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=8, shuffle=True, num_workers=10,\n",
    "        collate_fn=None)\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        test_ds, batch_size=8, shuffle=False, num_workers=10,\n",
    "        collate_fn=None)\n",
    "\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_other_cfg(CfgNode({\n",
    "        'SOLVER': {\n",
    "            'OPTIMIZING_METHOD': 'sgd',\n",
    "            'BASE_LR': 0.1,\n",
    "            'LR_POLICY': 'none',\n",
    "            'MOMENTUM': 0.9,\n",
    "            'WEIGHT_DECAY': 1e-4,\n",
    "        }\n",
    "    }))\n",
    "    \n",
    "    system = VideoClassificationLightningModule(optim_cfg=cfg)\n",
    "    trainer = pytorch_lightning.Trainer(\n",
    "        # callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")],\n",
    "        accelerator='gpu',\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=5)\n",
    "    trainer.fit(system, data_loader_train, data_loader_val)\n",
    "\n",
    "    trainer.test(system, data_loader_val)\n",
    "    return system.test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_validation(dataset, metrics_name='binary'):\n",
    "    seed = 22\n",
    "    cv_splits = KFold(n_splits=10, random_state=seed, shuffle=True).split(range(len(dataset)))\n",
    "\n",
    "    outputs = torch.empty((len(dataset),))\n",
    "    labels = torch.empty((len(dataset),), dtype=torch.int)\n",
    "    for f, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "        # create datasets\n",
    "        train_ds = Subset(dataset, train_idx)\n",
    "        test_ds = Subset(dataset, test_idx)\n",
    "\n",
    "        fold_outputs = do_fold(train_ds, test_ds)\n",
    "        outputs[test_idx] = fold_outputs['proba'].float()\n",
    "        labels[test_idx] = fold_outputs['labels'].int()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    run_metrics = get_metrics(outputs, labels, metrics_name)\n",
    "\n",
    "    return outputs, run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = pd.read_csv('../dataset/computational_examples.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>person</th>\n",
       "      <th>cam</th>\n",
       "      <th>hit_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>calibration</th>\n",
       "      <th>hash</th>\n",
       "      <th>ini_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>...</th>\n",
       "      <th>gt_offset</th>\n",
       "      <th>gt_laughter</th>\n",
       "      <th>is_laughter</th>\n",
       "      <th>confidence</th>\n",
       "      <th>intensity</th>\n",
       "      <th>attempt</th>\n",
       "      <th>pressed_key</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>rating_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>9c45e4f0c5442e796eb93e73e94dc6c2dfca7b9c4c54ff...</td>\n",
       "      <td>video</td>\n",
       "      <td>False</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "      <td>7360.29</td>\n",
       "      <td>7361.54</td>\n",
       "      <td>...</td>\n",
       "      <td>4.420238</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.336670</td>\n",
       "      <td>6.639973</td>\n",
       "      <td>7af591213b827db95c12c56e76e0b1fe518f2088d11aad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>bff9b86d833a595e6fe5a54f45093fa168cda45db1143e...</td>\n",
       "      <td>video</td>\n",
       "      <td>False</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "      <td>7360.29</td>\n",
       "      <td>7361.54</td>\n",
       "      <td>...</td>\n",
       "      <td>4.420238</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.569236</td>\n",
       "      <td>6.639973</td>\n",
       "      <td>25df21dc0f25e11a7c4aba77e502269d42a7bb548044f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>4198c11729cea33268040a725998f16478a6564d4af091...</td>\n",
       "      <td>audio</td>\n",
       "      <td>False</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "      <td>7360.29</td>\n",
       "      <td>7361.54</td>\n",
       "      <td>...</td>\n",
       "      <td>4.420238</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.103220</td>\n",
       "      <td>4.263220</td>\n",
       "      <td>2cb0148d83e939600a9e1d71872ba748334e4d8d0cafa0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2440</td>\n",
       "      <td>2440</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>a9760ede24043c59a0151b09a46e866fa43f74bd60b682...</td>\n",
       "      <td>audio</td>\n",
       "      <td>False</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "      <td>7360.29</td>\n",
       "      <td>7361.54</td>\n",
       "      <td>...</td>\n",
       "      <td>4.420238</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.783220</td>\n",
       "      <td>4.143220</td>\n",
       "      <td>b3cc8b0750211f5b5100f20641793ad043dc7cc823ad9a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>f4c9842cec7be99eeaaea36d0c7d077c4d5d94596dc731...</td>\n",
       "      <td>av</td>\n",
       "      <td>False</td>\n",
       "      <td>1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...</td>\n",
       "      <td>7360.29</td>\n",
       "      <td>7361.54</td>\n",
       "      <td>...</td>\n",
       "      <td>4.420238</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.792656</td>\n",
       "      <td>3.893757</td>\n",
       "      <td>bf6cd2aeaf7c77c2c2ff873e6f603b7d46cd64c74e9ebd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  person  cam  \\\n",
       "0             0           0      25    1   \n",
       "1          1947        1947      25    1   \n",
       "2           546         546      25    1   \n",
       "3          2440        2440      25    1   \n",
       "4          1058        1058      25    1   \n",
       "\n",
       "                                              hit_id condition  calibration  \\\n",
       "0  9c45e4f0c5442e796eb93e73e94dc6c2dfca7b9c4c54ff...     video        False   \n",
       "1  bff9b86d833a595e6fe5a54f45093fa168cda45db1143e...     video        False   \n",
       "2  4198c11729cea33268040a725998f16478a6564d4af091...     audio        False   \n",
       "3  a9760ede24043c59a0151b09a46e866fa43f74bd60b682...     audio        False   \n",
       "4  f4c9842cec7be99eeaaea36d0c7d077c4d5d94596dc731...        av        False   \n",
       "\n",
       "                                                hash  ini_time  end_time  ...  \\\n",
       "0  1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...   7360.29   7361.54  ...   \n",
       "1  1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...   7360.29   7361.54  ...   \n",
       "2  1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...   7360.29   7361.54  ...   \n",
       "3  1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...   7360.29   7361.54  ...   \n",
       "4  1170917790b51bc5a8dacacc4d8ed8c410b7ea6bb7ea4b...   7360.29   7361.54  ...   \n",
       "\n",
       "   gt_offset  gt_laughter  is_laughter  confidence  intensity  attempt  \\\n",
       "0   4.420238         True         True           7          4        0   \n",
       "1   4.420238         True         True           1          6        0   \n",
       "2   4.420238         True         True           7          5        0   \n",
       "3   4.420238         True         True           7          6        0   \n",
       "4   4.420238         True         True           7          7        0   \n",
       "\n",
       "   pressed_key     onset    offset  \\\n",
       "0         True  3.336670  6.639973   \n",
       "1         True  2.569236  6.639973   \n",
       "2         True  2.103220  4.263220   \n",
       "3         True  2.783220  4.143220   \n",
       "4         True  2.792656  3.893757   \n",
       "\n",
       "                                         rating_hash  \n",
       "0  7af591213b827db95c12c56e76e0b1fe518f2088d11aad...  \n",
       "1  25df21dc0f25e11a7c4aba77e502269d42a7bb548044f2...  \n",
       "2  2cb0148d83e939600a9e1d71872ba748334e4d8d0cafa0...  \n",
       "3  b3cc8b0750211f5b5100f20641793ad043dc7cc823ad9a...  \n",
       "4  bf6cd2aeaf7c77c2c2ff873e6f603b7d46cd64c74e9ebd...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jose/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | Net  | 33.6 M\n",
      "-------------------------------\n",
      "4.6 K     Trainable params\n",
      "33.6 M    Non-trainable params\n",
      "33.6 M    Total params\n",
      "134.596   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn 0, non bn 2, zero 0 no grad 330\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed46b0d14c164d1ebb8377063f9eea1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2139098aa94340929ed1eb415b082abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c92668f2964dd787b3da30140afe49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0634adf2fd414dbb83aab6669806c5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b001ffcab73b4d049f2146487ff757fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abbf2c099f04c888e1669ad3f2650b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fec1cf12d843a9ac3e5bbf6fd1b885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118614f51f084dfda2fe5dfeb94d491e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_auc            0.5378378378378379\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "for label_modality in ['video']:\n",
    "\n",
    "    filtered_examples = examples[examples['condition'] == label_modality]\n",
    "    filtered_examples['filename'] = filtered_examples['hash']+'.mp4'\n",
    "    video_path = os.path.join(cloud_data_path, 'laughter_data', 'ml_datasets', 'tight', 'video')\n",
    "\n",
    "    dataset = my_video_dataset_from_dataframe(\n",
    "        examples_df=filtered_examples,\n",
    "        video_path_prefix=video_path,\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", 2),\n",
    "        transform=get_kinetics_train_transform(32, 256, True),\n",
    "        decode_audio=False,\n",
    "        file_path_key='filename',\n",
    "        label_key='pressed_key'\n",
    "    )\n",
    "\n",
    "    res[label_modality] = []\n",
    "    for i in range(5):\n",
    "        _, metrics = do_cross_validation(dataset, metrics_name='binary')\n",
    "        res[label_modality].append(metrics)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jose.torch.hooks import FeatureRecorder, Signal\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_slowfast_feature_extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/lared_laughter/video/train.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared_laughter/video/train.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m recorder \u001b[39m=\u001b[39m FeatureRecorder()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared_laughter/video/train.ipynb#ch0000016vscode-remote?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m make_slowfast_feature_extractor()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared_laughter/video/train.ipynb#ch0000016vscode-remote?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mblocks[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mproj\u001b[39m.\u001b[39mregister_forward_hook(recorder\u001b[39m.\u001b[39mget_hook(\u001b[39m'\u001b[39m\u001b[39mproj_input\u001b[39m\u001b[39m'\u001b[39m, Signal\u001b[39m.\u001b[39mINPUT))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_slowfast_feature_extractor' is not defined"
     ]
    }
   ],
   "source": [
    "recorder = FeatureRecorder()\n",
    "model = make_slowfast_feature_extractor()\n",
    "model.blocks[-1].proj.register_forward_hook(recorder.get_hook('proj_input', Signal.INPUT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = pd.read_csv('../dataset/computational_examples.csv')\n",
    "examples = examples[examples['condition'] == 'av']\n",
    "examples['filename'] = examples['hash']+'.mp4'\n",
    "video_path = os.path.join(dataset_path, 'video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = my_video_dataset_from_dataframe(\n",
    "    examples_df=examples,\n",
    "    video_path_prefix=video_path,\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", 2),\n",
    "    transform=get_kinetics_val_transform(32, 256, True),\n",
    "    decode_audio=False,\n",
    "    file_path_key='filename',\n",
    "    label_key='pressed_key'\n",
    ")\n",
    "dl = torch.utils.data.DataLoader(\n",
    "        ds, batch_size=8, shuffle=False, num_workers=10,\n",
    "        collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 8, 256, 256]) torch.Size([8, 3, 32, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dl))['video']\n",
    "print(b[0].shape, b[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6350fb32d1514d4abd550f54bd864b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n",
      "torch.Size([8, 1, 2, 2, 2304])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "recorder.clear()\n",
    "for batch in tqdm(dl):\n",
    "    y_hat = model(batch['video'])\n",
    "recorder.store_as_dict('./features/slowfast_test.pkl', dict_keys=examples.hash.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ff63645dd16f55240e07095d3c46f4fac3f89ef16802cfaceca713f6cf38dfb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
