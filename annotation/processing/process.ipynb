{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pyperclip\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from lared_laughter.constants import annot_exp_path, dataset_path\n",
    "from utils import CovfeeParser, get_hit_stats, interp_30fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation processing\n",
    "\n",
    "This file processes the covfee outputs into a single dataframe that aggregates the results of the human annotation experiments.\n",
    "An additional pkl file is produced with the continuous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open examples\n",
    "laughter_examples_df = pd.read_csv(os.path.join(annot_exp_path, 'laughter_examples', 'examples_with_rect.csv'), index_col=0)\n",
    "speech_examples_df = pd.read_csv(os.path.join(annot_exp_path, 'speech_examples', 'examples_with_rect.csv'), index_col=0)\n",
    "calibration_examples_df = pd.read_csv(os.path.join(annot_exp_path, 'calibration_examples', 'examples.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughter_examples_df['onset_time']     = laughter_examples_df['ini_time'] - laughter_examples_df['_ini_time']\n",
    "laughter_examples_df['offset_time']    = laughter_examples_df['_end_time'] - laughter_examples_df['ini_time']\n",
    "speech_examples_df['onset_time']       = speech_examples_df['ini_time'] - speech_examples_df['_ini_time']\n",
    "speech_examples_df['offset_time']      = speech_examples_df['_end_time'] - speech_examples_df['ini_time']\n",
    "calibration_examples_df['onset_time']  = calibration_examples_df['ini_time'] - calibration_examples_df['_ini_time']\n",
    "calibration_examples_df['offset_time'] = calibration_examples_df['_end_time'] - calibration_examples_df['ini_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1684, 785, 6)\n"
     ]
    }
   ],
   "source": [
    "laughter_examples = {row['hash']: row.to_dict() for _, row in laughter_examples_df.iterrows()}\n",
    "speech_examples = {row['hash']: row.to_dict() for _, row in speech_examples_df.iterrows()}\n",
    "calibration_examples = {row['hash']: row.to_dict() for _, row in calibration_examples_df.iterrows()}\n",
    "print((len(laughter_examples), len(speech_examples), len(calibration_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path_1 = os.path.join(annot_exp_path, 'covfee1')\n",
    "results_path_2 = os.path.join(annot_exp_path, 'covfee2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open covfee results\n",
    "hits = json.load(open('../laughter2.covfee.json', 'rb'))\n",
    "parser = CovfeeParser(laughter_examples, speech_examples, calibration_examples, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the parser with only one HIT\n",
    "# hit_path = Path(os.path.join(results_path_1, '024a80de5a3a053d40e1b7f3086e70c34cd90fc2a9668ccc6c47f7293425a185'))\n",
    "# list(parser.parse_hit(hit_path)['tasks'].values())[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/c/Users/Jose/gdrive/data/lared_laughter/annotation_experiment_2/covfee1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/lared_laughter/annotation/processing/process.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared_laughter/annotation/processing/process.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m all_results_1 \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mparse_v1(results_path_1)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared_laughter/annotation/processing/process.ipynb#ch0000011vscode-remote?line=1'>2</a>\u001b[0m all_results_2 \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_v2(results_path_2)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/lared_laughter/annotation/processing/process.ipynb#ch0000011vscode-remote?line=2'>3</a>\u001b[0m all_results \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_results_1, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_results_2}\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/lared_laughter/annotation/processing/utils.py:207\u001b[0m, in \u001b[0;36mCovfeeParser.parse_v1\u001b[0;34m(self, results_path, data_col, version)\u001b[0m\n\u001b[1;32m    204\u001b[0m all_results \u001b[39m=\u001b[39m {}\n\u001b[1;32m    206\u001b[0m p \u001b[39m=\u001b[39m Path(results_path)\n\u001b[0;32m--> 207\u001b[0m \u001b[39mfor\u001b[39;00m i_dir, hit_path \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(p\u001b[39m.\u001b[39miterdir()):\n\u001b[1;32m    208\u001b[0m     instance_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(version) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(hit_path)\n\u001b[1;32m    209\u001b[0m     all_results[instance_id] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_hit(hit_path, data_col, version)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/pathlib.py:1122\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed:\n\u001b[1;32m   1121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_closed()\n\u001b[0;32m-> 1122\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m):\n\u001b[1;32m   1123\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m}:\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/c/Users/Jose/gdrive/data/lared_laughter/annotation_experiment_2/covfee1'"
     ]
    }
   ],
   "source": [
    "all_results_1 = parser.parse_v1(results_path_1)\n",
    "all_results_2 = parser.parse_v2(results_path_2)\n",
    "all_results = {**all_results_1, **all_results_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "annotations = {}\n",
    "for res in all_results.values():\n",
    "    for ex_hash, example in res['tasks'].items():\n",
    "        examples.append(res['tasks'][ex_hash])\n",
    "\n",
    "        try:\n",
    "            continuous_data = res['continuous'][ex_hash]\n",
    "        except KeyError:\n",
    "            print(f'Error in instance {example[\"instance_id\"]}')\n",
    "            continue\n",
    "\n",
    "        annot = interp_30fps(\n",
    "            continuous_data,\n",
    "            example_len=(example['_end_time']-example['_ini_time'])\n",
    "        )\n",
    "        annotations[(example['instance_id'], example['hash'], example['condition'])] = annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_df = pd.DataFrame.from_dict(examples)\n",
    "calibration_examples_df = examples_df[examples_df['calibration']]\n",
    "examples_df = examples_df[~examples_df['calibration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing modalities\n",
    "for hash in examples_df['hash'].unique():\n",
    "    hash_examples = examples_df[examples_df['hash'] == hash]\n",
    "    \n",
    "    video_examples = hash_examples[hash_examples['condition'] == 'video']\n",
    "    audio_examples = hash_examples[hash_examples['condition'] == 'audio']\n",
    "    av_examples = hash_examples[hash_examples['condition'] == 'av']\n",
    "\n",
    "    num_examples = min(len(video_examples), len(audio_examples), len(av_examples))\n",
    "    assert num_examples >= 2, hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction of delays\n",
    "\n",
    "Here I correct the delays using the Calibration samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_cont_delay(arr, delay):\n",
    "    # move the annotations forward and pad with zeroes at the end\n",
    "    delay_in_samples = round(delay * 30)\n",
    "    new_data = np.roll(arr, shift = -delay_in_samples)\n",
    "    new_data[-delay_in_samples:] = 0\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_calibration_onsets(examples_df):\n",
    "    calibration_examples_onsets = {\n",
    "        'bb6337eea970487ce9cd4ff26ea78c7acc6d5d1a355b7aa50029a3229f115b21': 1.54,\n",
    "        '68d229cf19eec82f37580265ea93892117dd5b559b04d22489da3593315f18e7': 3.10,\n",
    "        '64a92aea9395ace7ac9d60eab34911e419fc66610cd76f1e29df4b4fd16f230f': 3.35,\n",
    "        '85aac70ec91eb3be1b313b33e0b7828394bbe4e4edc6a956d1e7061dfc8b250e': 1.96,\n",
    "        'ced6e78fe7940c10fbc9d7c385273e68459ca399ccb668c8123cf5a66fa99819': 2.61,\n",
    "        'b438e94f7fdcc80ea7927e320946c7b407b7c850fe04baa60a07a5df7d92a711': 2.99\n",
    "    }\n",
    "\n",
    "    for hash in examples_df['hash'].unique():\n",
    "        examples_df.loc[examples_df['hash'] == hash, 'gt_onset'] = calibration_examples_onsets[hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_delays(examples_df_2, calibration_examples_df, annotations):\n",
    "    set_calibration_onsets(calibration_examples_df)\n",
    "\n",
    "    # examples_df.insert(0, 'c_onset', -1)\n",
    "    # examples_df.insert(0, 'c_offset', -1)\n",
    "    examples_df = examples_df_2.copy()\n",
    "    new_annotations = {}\n",
    "    \n",
    "    for instance_id in examples_df['instance_id'].unique():\n",
    "        instance_examples = examples_df[examples_df['instance_id'] == instance_id]\n",
    "\n",
    "        instance_calibration = calibration_examples_df[calibration_examples_df['instance_id'] == instance_id]\n",
    "\n",
    "        onset_delay = (instance_calibration['onset'] - instance_calibration['gt_onset']).to_numpy()\n",
    "        onset_delay = onset_delay[(onset_delay > 0) & (onset_delay < 1)]\n",
    "        annotator_delay = onset_delay.mean()\n",
    "        examples_df.loc[examples_df['instance_id'] == instance_id, 'c_onset'] = examples_df.loc[examples_df['instance_id'] == instance_id, 'onset'] - annotator_delay\n",
    "        examples_df.loc[examples_df['instance_id'] == instance_id, 'c_offset'] = examples_df.loc[examples_df['instance_id'] == instance_id, 'offset'] - annotator_delay\n",
    "        examples_df.loc[examples_df['instance_id'] == instance_id, 'onset_times'] = examples_df.loc[examples_df['instance_id'] == instance_id, 'onset_times'] - annotator_delay\n",
    "        examples_df.loc[examples_df['instance_id'] == instance_id, 'offset_times'] = examples_df.loc[examples_df['instance_id'] == instance_id, 'offset_times'] - annotator_delay\n",
    "\n",
    "        for id, row in instance_examples.iterrows():\n",
    "            key = (row['instance_id'], row['hash'], row['condition'])\n",
    "            new_annotations[key] = correct_cont_delay(annotations[key], annotator_delay)\n",
    "\n",
    "    return examples_df, new_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_examples, corr_annotations = correct_delays(examples_df, calibration_examples_df, annotations)\n",
    "assert len(corr_examples) == len(corr_annotations), f'{len(corr_examples)} != {len(corr_annotations)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize intensity and confidence\n",
    "\n",
    "Here I normalize intensity and confidence per annotator and per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.loc[examples['intensity'].isna(), 'intensity'] = 0\n",
    "examples.loc[~examples['pressed_key'], 'intensity'] = 0\n",
    "for instance_id in examples['instance_id'].unique():\n",
    "    for condition in examples['condition'].unique():\n",
    "        hit_condition_examples = examples[\n",
    "            (examples['instance_id'] == instance_id) & (examples['condition'] == condition)\n",
    "        ]\n",
    "        assert len(hit_condition_examples) in [28, 23, 24], len(hit_condition_examples)\n",
    "        idxs = (examples['instance_id'] == instance_id) & (examples['condition'] == condition)\n",
    "\n",
    "        for col in ['intensity', 'confidence']:\n",
    "            mean = examples.loc[idxs, col].mean()\n",
    "            examples.loc[idxs, 'norm_'+col] = examples.loc[idxs, col] - mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_examples['onset_times'] = [str(e.tolist()) for e in corr_examples['onset_times']]\n",
    "corr_examples['offset_times'] = [str(e.tolist()) for e in corr_examples['offset_times']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(annot_exp_path, 'processed')\n",
    "corr_examples.to_csv(os.path.join(out_path, 'examples_without_calibration.csv'))\n",
    "pickle.dump(corr_annotations, open(os.path.join(out_path, 'continuous_corrected.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIT stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_df = pd.DataFrame([get_hit_stats(hit) for hit in all_results.values()]).sort_values(by=['hit_group', 'hit_num'])\n",
    "hits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2642/237448400.py:9: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  pyperclip.copy(hits_df.to_latex(\n"
     ]
    }
   ],
   "source": [
    "hits_df['annot_id'] = range(1, len(hits_df)+1)\n",
    "hits_df['rating'] = hits_df['rating'].astype(float)\n",
    "hits_df['duration'] = hits_df['duration'].astype(float)\n",
    "hits_df['rating'].fillna(-1, inplace=True)\n",
    "def format_rating(f):\n",
    "    if f == -1: return '-'\n",
    "    return str(int(f))+'/5'\n",
    "    \n",
    "pyperclip.copy(hits_df.to_latex(\n",
    "    columns=['annot_id', 'hit_group', 'hit_num', 'pressed', 'intensities', 'confidences', 'duration', 'rating'], \n",
    "    header=['Annotator ID', 'G', 'N', '# positive', 'Intensity', 'Confidence', 'Time taken', 'Rating'],\n",
    "    formatters={'rating': format_rating, 'duration': '{:.1f}'.format},\n",
    "    index=False,\n",
    "    label='tab:hit_details',\n",
    "    caption='Details of the annotation HITs. \\\\textit{G} indicates the HIT group. HITs within the same group contain the same laughter/non-laughter samples. \\\\textit{N} indicates the HIT number within the group. HITs with the same \\\\textit{N} are identical, except for the (random) ordering of the samples within each condition. HITs with different \\\\textit{N} contain the same samples but assigned to different conditions. Each row corresponds to one annotator (HIT). \\\\textit{\\# positive} indicates the number of times laughter was detected by this person. \\\\textit{Intensities} indicates the histogram of laughter intensities by each annotator (positive examples only). Each number (in order) corresponds to one step in the Likert scale (1-7).'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIT information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 150)\n",
    "feedback_df = pd.DataFrame(hit_info).transpose()\n",
    "feedback_df['annot_id'] = range(1, len(hits_df)+1)\n",
    "feedback_df['feedback'].fillna('-', inplace=True)\n",
    "feedback_df['rating'] = feedback_df['rating'].astype(float)\n",
    "feedback_df['duration'] = feedback_df['duration'].astype(float)\n",
    "feedback_df['rating'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3954, 4242, 3954)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples), len(annotations), len(examples[examples['has_continuous'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>rating</th>\n",
       "      <th>feedback</th>\n",
       "      <th>annot_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2_7e49a08cdbcac9a9ad7658116b8a160dfa526831777981919e31172426ecc669</th>\n",
       "      <td>72.150000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_8bfd003817f3348a53d7292c84d0a155c4ec793a5c3ef2dfe2a5f6e0a272d505</th>\n",
       "      <td>41.950000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_96bc549cc2a7b0e3df2888482f5199982b80d32254f53df9f4684309d9181403</th>\n",
       "      <td>32.233333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The tools were working perfectly and the process was smooth.</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_bd45ce15933da0e06d4d544f27a95cb5de3147aaf592557feb5bae9fa0a5d583</th>\n",
       "      <td>39.400000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>At the beginning was a little bit complicated.</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_be99f777836ba4bffac2e20cc4e75a717d766fd5d2ccbe236dc1147d579b490b</th>\n",
       "      <td>54.566667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It was interesting to see how much difficult was to recognize the laughs of people when it was only video. Im confident a higher quality on it wou...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_c9c20f2eee48c8f1f8120bb25055b3bbb167d1f9284f31b9ee303f548f2da9bd</th>\n",
       "      <td>73.150000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I just found it a bit long but interesting and I did not have an issue with the tool.</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_da567d02513f82ff3b0fe0c5d6237e4226ffadf44f1d9897a2efea683c0479ff</th>\n",
       "      <td>35.483333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_dc19285db2b452c1d3a7d625dcf0f302ee9e2eb3f690bddcddaf7ae22e58335f</th>\n",
       "      <td>40.050000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It was a kinda long but not too much.</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_eda1bf4c0b1962f12a7a92dbb3159f85887d48334e764f225aeedee9b4b7db16</th>\n",
       "      <td>33.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_ff9b4ba9e0d4e962b5a1c5f584409289edb280057543ca3c98cedf9a1d31b0eb</th>\n",
       "      <td>60.266667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It was indeed a unique, one-of-a-kind research, although being a bit long, I felt alright generally. A \"break\" of some sort in the middle would be...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     duration  \\\n",
       "2_7e49a08cdbcac9a9ad7658116b8a160dfa526831777981919e31172426ecc669  72.150000   \n",
       "2_8bfd003817f3348a53d7292c84d0a155c4ec793a5c3ef2dfe2a5f6e0a272d505  41.950000   \n",
       "2_96bc549cc2a7b0e3df2888482f5199982b80d32254f53df9f4684309d9181403  32.233333   \n",
       "2_bd45ce15933da0e06d4d544f27a95cb5de3147aaf592557feb5bae9fa0a5d583  39.400000   \n",
       "2_be99f777836ba4bffac2e20cc4e75a717d766fd5d2ccbe236dc1147d579b490b  54.566667   \n",
       "2_c9c20f2eee48c8f1f8120bb25055b3bbb167d1f9284f31b9ee303f548f2da9bd  73.150000   \n",
       "2_da567d02513f82ff3b0fe0c5d6237e4226ffadf44f1d9897a2efea683c0479ff  35.483333   \n",
       "2_dc19285db2b452c1d3a7d625dcf0f302ee9e2eb3f690bddcddaf7ae22e58335f  40.050000   \n",
       "2_eda1bf4c0b1962f12a7a92dbb3159f85887d48334e764f225aeedee9b4b7db16  33.400000   \n",
       "2_ff9b4ba9e0d4e962b5a1c5f584409289edb280057543ca3c98cedf9a1d31b0eb  60.266667   \n",
       "\n",
       "                                                                    rating  \\\n",
       "2_7e49a08cdbcac9a9ad7658116b8a160dfa526831777981919e31172426ecc669     4.0   \n",
       "2_8bfd003817f3348a53d7292c84d0a155c4ec793a5c3ef2dfe2a5f6e0a272d505    -1.0   \n",
       "2_96bc549cc2a7b0e3df2888482f5199982b80d32254f53df9f4684309d9181403     5.0   \n",
       "2_bd45ce15933da0e06d4d544f27a95cb5de3147aaf592557feb5bae9fa0a5d583     4.0   \n",
       "2_be99f777836ba4bffac2e20cc4e75a717d766fd5d2ccbe236dc1147d579b490b     5.0   \n",
       "2_c9c20f2eee48c8f1f8120bb25055b3bbb167d1f9284f31b9ee303f548f2da9bd     4.0   \n",
       "2_da567d02513f82ff3b0fe0c5d6237e4226ffadf44f1d9897a2efea683c0479ff    -1.0   \n",
       "2_dc19285db2b452c1d3a7d625dcf0f302ee9e2eb3f690bddcddaf7ae22e58335f     5.0   \n",
       "2_eda1bf4c0b1962f12a7a92dbb3159f85887d48334e764f225aeedee9b4b7db16     5.0   \n",
       "2_ff9b4ba9e0d4e962b5a1c5f584409289edb280057543ca3c98cedf9a1d31b0eb     5.0   \n",
       "\n",
       "                                                                                                                                                                                                                 feedback  \\\n",
       "2_7e49a08cdbcac9a9ad7658116b8a160dfa526831777981919e31172426ecc669                                                                                                                                                      -   \n",
       "2_8bfd003817f3348a53d7292c84d0a155c4ec793a5c3ef2dfe2a5f6e0a272d505                                                                                                                                                      -   \n",
       "2_96bc549cc2a7b0e3df2888482f5199982b80d32254f53df9f4684309d9181403                                                                                           The tools were working perfectly and the process was smooth.   \n",
       "2_bd45ce15933da0e06d4d544f27a95cb5de3147aaf592557feb5bae9fa0a5d583                                                                                                         At the beginning was a little bit complicated.   \n",
       "2_be99f777836ba4bffac2e20cc4e75a717d766fd5d2ccbe236dc1147d579b490b  It was interesting to see how much difficult was to recognize the laughs of people when it was only video. Im confident a higher quality on it wou...   \n",
       "2_c9c20f2eee48c8f1f8120bb25055b3bbb167d1f9284f31b9ee303f548f2da9bd                                                                 I just found it a bit long but interesting and I did not have an issue with the tool.    \n",
       "2_da567d02513f82ff3b0fe0c5d6237e4226ffadf44f1d9897a2efea683c0479ff                                                                                                                                                      -   \n",
       "2_dc19285db2b452c1d3a7d625dcf0f302ee9e2eb3f690bddcddaf7ae22e58335f                                                                                                                  It was a kinda long but not too much.   \n",
       "2_eda1bf4c0b1962f12a7a92dbb3159f85887d48334e764f225aeedee9b4b7db16                                                                                                                                                      -   \n",
       "2_ff9b4ba9e0d4e962b5a1c5f584409289edb280057543ca3c98cedf9a1d31b0eb  It was indeed a unique, one-of-a-kind research, although being a bit long, I felt alright generally. A \"break\" of some sort in the middle would be...   \n",
       "\n",
       "                                                                    annot_id  \n",
       "2_7e49a08cdbcac9a9ad7658116b8a160dfa526831777981919e31172426ecc669        39  \n",
       "2_8bfd003817f3348a53d7292c84d0a155c4ec793a5c3ef2dfe2a5f6e0a272d505        40  \n",
       "2_96bc549cc2a7b0e3df2888482f5199982b80d32254f53df9f4684309d9181403        41  \n",
       "2_bd45ce15933da0e06d4d544f27a95cb5de3147aaf592557feb5bae9fa0a5d583        42  \n",
       "2_be99f777836ba4bffac2e20cc4e75a717d766fd5d2ccbe236dc1147d579b490b        43  \n",
       "2_c9c20f2eee48c8f1f8120bb25055b3bbb167d1f9284f31b9ee303f548f2da9bd        44  \n",
       "2_da567d02513f82ff3b0fe0c5d6237e4226ffadf44f1d9897a2efea683c0479ff        45  \n",
       "2_dc19285db2b452c1d3a7d625dcf0f302ee9e2eb3f690bddcddaf7ae22e58335f        46  \n",
       "2_eda1bf4c0b1962f12a7a92dbb3159f85887d48334e764f225aeedee9b4b7db16        47  \n",
       "2_ff9b4ba9e0d4e962b5a1c5f584409289edb280057543ca3c98cedf9a1d31b0eb        48  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1949/3810216713.py:2: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  feedback_df.to_latex(\n"
     ]
    }
   ],
   "source": [
    "pyperclip.copy(\n",
    "    feedback_df.to_latex(\n",
    "    columns=['annot_id', 'duration', 'rating', 'feedback'], \n",
    "    header=['Annotator ID', 'Time taken (s)', 'Rating', 'Feedback'],\n",
    "    # float_format=\"{:.1f}\".format,\n",
    "    formatters={'rating': format_rating, 'duration': '{:.1f}'.format},\n",
    "    column_format='',\n",
    "    index=False,\n",
    "    label='tab:hit_feedback',\n",
    "    caption='Feedback given by annotators after completing the experiment.'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ff63645dd16f55240e07095d3c46f4fac3f89ef16802cfaceca713f6cf38dfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
