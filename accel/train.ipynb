{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn import MyAlexNet\n",
    "from models.resnet import ResNetBaseline\n",
    "from jose.my_torch.helpers import train_one_epoch, eval, test, test_binary, test_regression, _test\n",
    "from jose.eval.eval import dataset_train_test\n",
    "from dataset import AccelLaughterDataset\n",
    "from constants import cloud_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 672 examples\n",
      "467 have accel\n"
     ]
    }
   ],
   "source": [
    "examples = pd.read_csv('../dataset/computational_examples.csv')\n",
    "examples = examples[examples['condition'] == 'av']\n",
    "accel_ds_path = os.path.join(cloud_data_path, 'accel', 'accel_ds.pkl')\n",
    "ds = AccelLaughterDataset(examples, accel_ds_path, label='pressed_key', example_len=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 359)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(ds.examples_df), len(ds.accel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.accel['006f74addfc99845bf6c9f80d13d52ccc189341031525530762bb83dd8b713af'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(outputs, labels, model):\n",
    "    if model in ['bce']:\n",
    "        proba = torch.sigmoid(outputs)\n",
    "        pred = (proba > 0.5)\n",
    "\n",
    "        correct = pred.eq(outputs.bool()).sum().item()\n",
    "        return {\n",
    "            'auc': roc_auc_score(labels, proba),\n",
    "            'correct': correct\n",
    "        }\n",
    "    elif model in ['l1', 'mse', 'mean_baseline']:\n",
    "        return {\n",
    "            'mse': torch.nn.functional.mse_loss(outputs, labels, reduction='mean'),\n",
    "            'l1': torch.nn.functional.l1_loss(outputs, labels, reduction='mean')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fold(train_idx, test_idx, model='bce', logfile=None):\n",
    "    # create datasets    \n",
    "    train_ds = Subset(ds, train_idx)\n",
    "    test_ds = Subset(ds, test_idx)\n",
    "    \n",
    "    # data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=100, shuffle=True, num_workers=0,\n",
    "        collate_fn=None)\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        test_ds, batch_size=100, shuffle=False, num_workers=0,\n",
    "        collate_fn=None)\n",
    "\n",
    "    if model in ['bce', 'l1', 'mse']:\n",
    "        return do_fold_cnn(data_loader, data_loader_val, model, logfile)\n",
    "    elif model in ['mean_baseline']:\n",
    "        return do_fold_mean_baseline(data_loader, data_loader_val)\n",
    "\n",
    "def do_fold_mean_baseline(train_dl, test_dl):\n",
    "    labels = []\n",
    "    for batch_idx, (X, Y) in enumerate(train_dl):\n",
    "        Y = Y.float()\n",
    "        labels.append(Y.reshape(-1))\n",
    "\n",
    "    labels = torch.cat(labels)\n",
    "    return torch.full((len(test_dl.dataset),), labels.mean().item())\n",
    "\n",
    "def do_fold_cnn(train_dl, test_dl, loss='bce', logfile=None):\n",
    "    \n",
    "    # model = MyAlexNet()\n",
    "    model = ResNetBaseline(in_channels = 3)\n",
    "    if loss == 'bce':\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    elif loss == 'mse':\n",
    "        loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    elif loss == 'l1':\n",
    "        loss_fn = torch.nn.L1Loss(reduction='sum')\n",
    "    else:\n",
    "        raise Exception('unknown loss')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        try:\n",
    "            train_one_epoch(model, loss_fn, device, train_dl, optimizer, epoch)\n",
    "            eval_labels, eval_output, stats = _test(model, loss_fn, device, test_dl)\n",
    "            eval_metrics = get_metrics(eval_output, eval_labels, loss)\n",
    "            if logfile is not None:\n",
    "                logfile.write(str(eval_metrics)+'\\n')\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        \n",
    "    # testing\n",
    "    all_labels, all_output, stats = _test(model, loss_fn, device, test_dl)\n",
    "\n",
    "    metrics = get_metrics(all_output, all_labels, loss)\n",
    "    return all_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_run(model):\n",
    "    seed = 22\n",
    "    cv_splits = KFold(n_splits=10, random_state=seed, shuffle=True).split(range(len(ds)))\n",
    "\n",
    "    fh = open('run_logs.log', 'w')\n",
    "\n",
    "    outputs = torch.empty((len(ds),))\n",
    "    for f, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "        fold_outputs = do_fold(train_idx, test_idx, model, logfile=fh)\n",
    "        outputs[test_idx] = fold_outputs.cpu()\n",
    "\n",
    "    labels = torch.Tensor(ds.get_all_labels())\n",
    "    run_metrics = get_metrics(outputs, labels, model)\n",
    "    fh.close()\n",
    "    print(run_metrics)\n",
    "\n",
    "    return outputs, run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.7926544966018652, 'correct': 330}\n",
      "{'auc': 0.7938794057215111, 'correct': 316}\n",
      "{'auc': 0.7698948948948949, 'correct': 328}\n",
      "{'auc': 0.8161450924608818, 'correct': 345}\n",
      "{'auc': 0.808854907539118, 'correct': 336}\n",
      "{'auc': 0.8093488225067172, 'correct': 327}\n",
      "{'auc': 0.7558479532163743, 'correct': 347}\n",
      "{'auc': 0.8108503240082188, 'correct': 337}\n",
      "{'auc': 0.8005768926821558, 'correct': 318}\n",
      "{'auc': 0.8224276908487435, 'correct': 340}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7980480480480481, 0.019891141375635555)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = []\n",
    "for i in range(10):\n",
    "    _, run_metrics = do_run('bce')\n",
    "    perf.append(run_metrics['auc'])\n",
    "    \n",
    "(np.mean(perf), np.std(perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
